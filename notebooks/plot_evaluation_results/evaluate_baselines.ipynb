{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from panda.utils.eval_utils import get_summary_metrics_dict\n",
    "from panda.utils.plot_utils import (\n",
    "    apply_custom_style,\n",
    "    make_box_plot,\n",
    "    plot_all_metrics_by_prediction_length,\n",
    ")\n",
    "\n",
    "apply_custom_style(\"../../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COLORS = list(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../../figures\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)\n",
    "\n",
    "outputs_save_dir = os.path.join(\"../../outputs\", \"eval_metrics\")\n",
    "os.makedirs(outputs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"test_zeroshot\"\n",
    "\n",
    "run_metrics_dir_dict = {\n",
    "    \"Panda\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"patchtst\",\n",
    "        \"pft_chattn_emb_w_poly-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M SFT\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        # \"chronos_nondeterministic\",\n",
    "        \"chronos_t5_mini_ft-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        # \"chronos_nondeterministic\",\n",
    "        \"chronos\",\n",
    "        \"chronos_mini_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        # \"chronos_nondeterministic\",\n",
    "        \"chronos_base_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Time MOE 50M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timemoe\",\n",
    "        \"timemoe-50m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"TimesFM 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timesfm\",\n",
    "        \"timesfm-200m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Dynamix\": os.path.join(WORK_DIR, \"eval_results\", \"dynamix\", data_split),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Pattern to find np.float64(...) values or plain numbers/special values\n",
    "_VALUE_PATTERN = re.compile(r\"np\\.float\\d*\\(([^)]+)\\)|([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?|nan|inf|-inf)\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _parse_float(s: str) -> float:\n",
    "    \"\"\"Parse a string to float, handling nan/inf.\"\"\"\n",
    "    s = s.strip().lower()\n",
    "    if s == \"nan\":\n",
    "        return np.nan\n",
    "    elif s in {\"inf\", \"+inf\"}:\n",
    "        return np.inf\n",
    "    elif s == \"-inf\":\n",
    "        return -np.inf\n",
    "    return float(s)\n",
    "\n",
    "\n",
    "def parse_metric_lists(metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def parse_value(value):\n",
    "        if isinstance(value, str) and value.strip().startswith(\"[\"):\n",
    "            return [_parse_float(m.group(1) or m.group(2)) for m in _VALUE_PATTERN.finditer(value)]\n",
    "        return value\n",
    "\n",
    "    parsed = metrics_df.copy()\n",
    "    for col in parsed.columns:\n",
    "        if col != \"system\":\n",
    "            parsed[col] = parsed[col].apply(parse_value)\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def aggregate_system_metrics(metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def agg(value):\n",
    "        return (\n",
    "            float(np.nanmean(value))\n",
    "            if isinstance(value, list) and value\n",
    "            else (np.nan if isinstance(value, list) else value)\n",
    "        )\n",
    "\n",
    "    aggregated = metrics_df.copy()\n",
    "    for col in aggregated.columns:\n",
    "        if col != \"system\":\n",
    "            aggregated[col] = aggregated[col].apply(agg)\n",
    "    return aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs = defaultdict(dict)\n",
    "instance_metrics_all_runs = defaultdict(dict)\n",
    "for model_name, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "    print(model_name)\n",
    "    if not os.path.exists(run_metrics_dir):\n",
    "        print(f\"Run metrics dir does not exist: {run_metrics_dir}\")\n",
    "        continue\n",
    "    for file in sorted(\n",
    "        filter(lambda x: x.endswith(\".csv\"), os.listdir(run_metrics_dir)),\n",
    "        key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "    ):\n",
    "        if file.endswith(\".csv\"):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            metrics_df = pd.read_csv(os.path.join(run_metrics_dir, file))\n",
    "            parsed_metrics = parse_metric_lists(metrics_df)\n",
    "            aggregated_metrics = aggregate_system_metrics(parsed_metrics)\n",
    "            metrics_all_runs[model_name][prediction_length] = aggregated_metrics.to_dict()\n",
    "            instance_metrics_all_runs[model_name][prediction_length] = parsed_metrics.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics_all_runs[\"Chronos 20M SFT\"][64][\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unroll metrics from nested structure\n",
    "unrolled_metrics = {\n",
    "    model: {\n",
    "        pred_len: {k: list(v.values()) for k, v in metrics.items() if k != \"system\"}\n",
    "        for pred_len, metrics in model_metrics.items()\n",
    "    }\n",
    "    for model, model_metrics in metrics_all_runs.items()\n",
    "}\n",
    "\n",
    "# Sort by median smape at prediction length 128 (excludes models without this data)\n",
    "median_smape_128 = lambda m: float(np.nanmedian(unrolled_metrics[m][128][\"smape\"]))\n",
    "sorted_models = sorted(\n",
    "    (m for m in unrolled_metrics if 128 in unrolled_metrics[m] and \"smape\" in unrolled_metrics[m][128]),\n",
    "    key=median_smape_128,\n",
    ")\n",
    "unrolled_metrics = {m: unrolled_metrics[m] for m in sorted_models}\n",
    "n_runs = len(unrolled_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Chronos 20M SFT\"][128][\"smape\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "METRICS = [\"mae\", \"mse\", \"smape\"]\n",
    "PRED_LENGTHS = [128, 256, 512]\n",
    "\n",
    "# Build results dictionary\n",
    "p_values = defaultdict(lambda: defaultdict(dict))\n",
    "for baseline, metrics_by_len in unrolled_metrics.items():\n",
    "    if baseline == \"Panda\":\n",
    "        continue\n",
    "    for pred_len, metrics in metrics_by_len.items():\n",
    "        for metric in METRICS:\n",
    "            baseline_vals = np.array(metrics[metric])\n",
    "            panda_vals = np.array(unrolled_metrics[\"Panda\"][pred_len][metric])\n",
    "            valid = ~np.isnan(baseline_vals)\n",
    "            stat, pval = wilcoxon(panda_vals[valid], baseline_vals[valid], correction=True)\n",
    "            p_values[pred_len][f\"{metric}_pvalue\"][baseline] = pval\n",
    "            p_values[pred_len][f\"{metric}_statistic\"][baseline] = stat\n",
    "\n",
    "# Process and save results for each prediction length\n",
    "for pred_len in PRED_LENGTHS:\n",
    "    df = pd.DataFrame(p_values[pred_len]).dropna()\n",
    "    for col in df.columns:\n",
    "        if \"pvalue\" in col and len(df[col]) > 0:\n",
    "            rejected, pvals_adj, *_ = multipletests(df[col])\n",
    "            df[f\"{col}_pval_adj\"], df[f\"{col}_reject\"] = pvals_adj, rejected\n",
    "    df.to_csv(f\"{outputs_save_dir}/pvals_{pred_len}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = DEFAULT_COLORS[: n_runs + 1]\n",
    "default_colors = default_colors[:3] + default_colors[4:7]\n",
    "print(default_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"smape\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"{figs_save_dir}/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(5, 95),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"mae\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"{figs_save_dir}/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(5, 90),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=6,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_horizontal_patches.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=1,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_vertical_patches.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict, has_nans = get_summary_metrics_dict(unrolled_metrics, \"smape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mse\", \"mae\", \"smape\", \"spearman\"]\n",
    "metrics_dicts, has_nans = zip(*[get_summary_metrics_dict(unrolled_metrics, metric) for metric in metrics])\n",
    "all_metrics_dict = {m: metrics_dicts[i] for i, m in enumerate(metrics)}\n",
    "has_nans_dict = {m: has_nans[i] for i, m in enumerate(metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaNs for each metric and model\n",
    "nan_counts = {}\n",
    "for metric_name, metric_data in all_metrics_dict.items():\n",
    "    nan_counts[metric_name] = {}\n",
    "    for model_name, model_data in metric_data.items():\n",
    "        # all_vals = np.concatenate(model_data[\"all_vals\"])\n",
    "        all_vals_pred128 = model_data[\"all_vals\"][1]\n",
    "        nan_count = np.isnan(all_vals_pred128).sum()\n",
    "        nan_counts[metric_name][model_name] = nan_count\n",
    "        if nan_count > 0:\n",
    "            print(f\"Found {nan_count} NaNs in {model_name} for {metric_name}\")\n",
    "\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order model names by sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_ordering = []  # sorted by median smape at 128\n",
    "for model_name, data in all_metrics_dict[\"smape\"].items():\n",
    "    median_metrics_128 = data[\"medians\"][1]\n",
    "    model_names_ordering.append((model_name, median_metrics_128))\n",
    "model_names_ordering = sorted(model_names_ordering, key=lambda x: x[1])\n",
    "model_names_ordering = [x[0] for x in model_names_ordering]\n",
    "print(model_names_ordering)\n",
    "\n",
    "# Reorder all_metrics_dict according to model_names_ordering for each metric\n",
    "reordered_metrics_dict = {}\n",
    "for metric_name, metric_data in all_metrics_dict.items():\n",
    "    reordered_metric_data = {}\n",
    "\n",
    "    # Add models in the order specified by model_names_ordering\n",
    "    for model_name in model_names_ordering:\n",
    "        if model_name in metric_data:\n",
    "            reordered_metric_data[model_name] = metric_data[model_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not found in {metric_name}\")\n",
    "\n",
    "    reordered_metrics_dict[metric_name] = reordered_metric_data\n",
    "all_metrics_dict = reordered_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pred_lengths = [128, 256, 512]\n",
    "all_pred_lengths = list(unrolled_metrics[\"Panda\"].keys())\n",
    "print(all_pred_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in unrolled_metrics.keys():\n",
    "    print(f\"========= model_name: {model_name}\")\n",
    "    for i, pred_length in enumerate(all_pred_lengths):\n",
    "        if pred_length not in selected_pred_lengths:\n",
    "            continue\n",
    "        print(f\"pred_length: {pred_length}\")\n",
    "        for metric in [\"mae\"]:\n",
    "            print(f\"{metric} median: {all_metrics_dict[metric][model_name]['medians'][i]:.2f}\")\n",
    "            print(f\"{metric} p25: {all_metrics_dict[metric][model_name]['p25'][i]:.2f}\")\n",
    "            print(f\"{metric} p75: {all_metrics_dict[metric][model_name]['p75'][i]:.2f}\")\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    metrics_to_show_envelope=[\"mae\", \"smape\", \"spearman\"],\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_metrics_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"loc\": \"upper left\", \"frameon\": True, \"fontsize\": 10},\n",
    "    colors=default_colors,\n",
    "    use_inv_spearman=True,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles.values(),\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=6,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_horizontal.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"smape\"\n",
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles.values(),\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=1,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_vertical.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"mae\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"mse\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    # metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    # legend_kwargs={\"frameon\": True, \"fontsize\": 12, \"loc\": \"upper left\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"spearman\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    "    use_inv_spearman=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
