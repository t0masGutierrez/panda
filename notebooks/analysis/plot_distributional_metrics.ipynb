{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Metrics Analysis\n",
    "\n",
    "This notebook analyzes and visualizes distributional metrics across multiple context windows and metrics computation runs.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook processes distributional metrics (Hellinger distance and KL divergence) from multiple evaluation result directories, accumulates them across different prediction windows, and generates comparative visualizations and statistics.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### 1. **Multi-Directory Data Loading**\n",
    "- Loads metrics from multiple evaluation directories specified in `eval_results_dir_lst`\n",
    "- Automatically discovers and sorts metric files by prediction horizon, then accumulates the average across all context windows.\n",
    "\n",
    "### 2. **Models Analyzed**\n",
    "- **Panda**: 21M parameter Panda model\n",
    "- **Chronos 20M SFT**: Fine-tuned Chronos 20M model\n",
    "- **Chronos 20M**: Chronos 20M model\n",
    "- **Chronos 200M**: Chronos 200M model\n",
    "- **DynaMix**: DynaMix model\n",
    "\n",
    "### 3. **Metrics Computed**\n",
    "- **Average Hellinger Distance**: Average per-dimension spectral Hellinger distance, using the implementation from the DynaMix authors.\n",
    "- **KL Divergence**: Geometric misalignment, using the implementation from DynaMix authors.\n",
    "- **Prediction Time**: Computational efficiency metrics (Currenlty not written to, so will always be 0.0).\n",
    "\n",
    "### 4. **Analysis Types**\n",
    "- Distribution histograms for individual metrics\n",
    "- Model-to-model comparisons (differences in KLD and Hellinger distance)\n",
    "- Statistics across multiple prediction horizons (512, 1024, 2048, 3072, 3584)\n",
    "- Support for both prediction horizon and full trajectory evaluations\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Key parameters:\n",
    "- `eval_results_dir_lst`: List of directories containing evaluation results\n",
    "- `data_split`: Data split to analyze (default: \"test_zeroshot\")\n",
    "- `run_name`: Optional run suffix for organizing results (default: \"fdiv\")\n",
    "- `use_chronos_deterministic`: Flag for deterministic vs non-deterministic Chronos results\n",
    "\n",
    "## Output\n",
    "\n",
    "- Figures saved to `../../figures/eval_metrics/`\n",
    "- Statistical summaries printed to stdout\n",
    "- Mean ± standard deviation for all metrics across prediction horizons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from panda.utils.plot_utils import apply_custom_style\n",
    "\n",
    "apply_custom_style(\"../../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_save_dir = os.path.join(\"../../figures\", \"eval_metrics\")\n",
    "os.makedirs(fig_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COLORS = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")\n",
    "# eval_results_dir = os.path.join(WORK_DIR, \"eval_results_distributional_long\")\n",
    "eval_results_dir_lst = [\n",
    "    os.path.join(WORK_DIR, \"eval_results_distributional_long\"),\n",
    "    os.path.join(WORK_DIR, \"eval_results_distributional_3072\"),\n",
    "    # os.path.join(WORK_DIR, \"old_eval_results_backup/eval_results_distributional_longest\"),\n",
    "    # os.path.join(WORK_DIR, \"old_eval_results_backup/eval_results_distributional\"),\n",
    "]\n",
    "data_split = \"test_zeroshot\"\n",
    "run_name = \"fdiv\"\n",
    "# run_name = \"fdiv_kld-gmm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_chronos_deterministic = True\n",
    "chronos_dirname = \"chronos\" if use_chronos_deterministic else \"chronos_nondeterministic\"\n",
    "\n",
    "print(f\"Using {chronos_dirname} for chronos metrics\")\n",
    "\n",
    "\n",
    "def get_sorted_metric_fnames(save_dir):\n",
    "    def extract_window(fname):\n",
    "        m = re.search(r\"window-(\\d+)\", fname)\n",
    "        return int(m.group(1)) if m else float(\"inf\")\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        return []\n",
    "\n",
    "    return sorted(\n",
    "        [f for f in os.listdir(save_dir) if f.endswith(\".json\") and \"distributional_metrics\" in f], key=extract_window\n",
    "    )\n",
    "\n",
    "\n",
    "run_suffix = run_name if run_name else \"\"\n",
    "\n",
    "# Define model paths relative to eval_results_dir\n",
    "model_path_templates = {\n",
    "    \"Panda\": (\"panda\", \"panda-21M\", data_split, run_suffix),\n",
    "    \"Chronos 20M SFT\": (chronos_dirname, \"chronos_t5_mini_ft-0\", data_split, run_suffix),\n",
    "    \"Chronos 20M\": (chronos_dirname, \"chronos_mini_zeroshot\", data_split, run_suffix),\n",
    "    \"Chronos 200M\": (chronos_dirname, \"chronos_base_zeroshot\", data_split, run_suffix),\n",
    "    \"DynaMix\": (\"dynamix\", \"dynamix\", data_split, run_suffix),\n",
    "}\n",
    "model_run_names = list(model_path_templates.keys())\n",
    "\n",
    "# Collect all directories and filenames for each model across all eval_results_dirs\n",
    "metrics_dirs_and_fnames = {model_name: [] for model_name in model_run_names}\n",
    "\n",
    "for eval_results_dir in eval_results_dir_lst:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Processing eval_results_dir: {eval_results_dir}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    for model_name, path_parts in model_path_templates.items():\n",
    "        save_dir = os.path.join(eval_results_dir, *path_parts)\n",
    "        print(f\"Loading {model_name} metrics from: {save_dir}\")\n",
    "        found_fnames = get_sorted_metric_fnames(save_dir)\n",
    "\n",
    "        if found_fnames:\n",
    "            metrics_dirs_and_fnames[model_name].append({\"save_dir\": save_dir, \"fnames\": found_fnames})\n",
    "            print(f\"  Found {len(found_fnames)} files: {found_fnames}\")\n",
    "        else:\n",
    "            print(\"  No metrics files found\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Summary of collected metrics:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "for model_name in model_run_names:\n",
    "    total_files = sum(len(entry[\"fnames\"]) for entry in metrics_dirs_and_fnames[model_name])\n",
    "    total_dirs = len(metrics_dirs_and_fnames[model_name])\n",
    "    print(f\"{model_name}: {total_files} files from {total_dirs} director(ies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example metrics file\n",
    "# Get the first directory and filename for Panda\n",
    "first_dir_entry = metrics_dirs_and_fnames[\"Panda\"][0]\n",
    "metrics_fpath = os.path.join(first_dir_entry[\"save_dir\"], first_dir_entry[\"fnames\"][0])\n",
    "with open(metrics_fpath, \"rb\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Convert string keys to integers\n",
    "metrics = {int(k): v for k, v in metrics.items()}\n",
    "\n",
    "print(metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_metrics(dirs_and_fnames_list):\n",
    "    \"\"\"Accumulate distributional metrics across multiple files and directories.\n",
    "\n",
    "    Args:\n",
    "        dirs_and_fnames_list: List of dicts with 'save_dir' and 'fnames' keys\n",
    "    \"\"\"\n",
    "    HORIZONS = [\"prediction_horizon\", \"full_trajectory\"]\n",
    "    METRICS = [\"avg_hellinger_distance\", \"kl_divergence\"]\n",
    "\n",
    "    # Initialize accumulators\n",
    "    accum = {metric: {horizon: defaultdict(lambda: defaultdict(list)) for horizon in HORIZONS} for metric in METRICS}\n",
    "\n",
    "    # Accumulate values from all directories and files\n",
    "    for dir_entry in dirs_and_fnames_list:\n",
    "        metrics_save_dir = dir_entry[\"save_dir\"]\n",
    "        metrics_fnames = dir_entry[\"fnames\"]\n",
    "\n",
    "        print(f\"\\n  Processing directory: {metrics_save_dir}\")\n",
    "\n",
    "        for fname in metrics_fnames:\n",
    "            with open(os.path.join(metrics_save_dir, fname), \"rb\") as f:\n",
    "                metrics = json.load(f)\n",
    "            metrics = {int(k) if isinstance(k, str) else k: v for k, v in metrics.items()}\n",
    "\n",
    "            print(f\"    Processing {fname}: {len(metrics)} prediction interval(s)\")\n",
    "\n",
    "            for pred_interval, data in metrics.items():\n",
    "                for system_name, system_entry in tqdm(data, desc=f\"    Interval {pred_interval}\"):\n",
    "                    # Process each horizon\n",
    "                    for horizon in HORIZONS:\n",
    "                        if horizon in system_entry:\n",
    "                            for metric in METRICS:\n",
    "                                accum[metric][horizon][pred_interval][system_name].append(system_entry[horizon][metric])\n",
    "\n",
    "    # Compute means, filtering None values\n",
    "    def compute_means(data_accum):\n",
    "        result = {horizon: defaultdict(dict) for horizon in HORIZONS}\n",
    "        for horizon in HORIZONS:\n",
    "            for pred_interval, systems in data_accum[horizon].items():\n",
    "                for system_name, values in systems.items():\n",
    "                    filtered = [v for v in values if v is not None]\n",
    "                    result[horizon][pred_interval][system_name] = float(np.mean(filtered)) if filtered else None\n",
    "        return result\n",
    "\n",
    "    return {\n",
    "        \"avg_hellinger\": compute_means(accum[\"avg_hellinger_distance\"]),\n",
    "        \"kld\": compute_means(accum[\"kl_divergence\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics_by_modelname = {}\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Accumulating metrics for all models:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "for model_name in model_run_names:\n",
    "    print(f\"\\nAccumulating {model_name} metrics...\")\n",
    "    metrics = accumulate_metrics(metrics_dirs_and_fnames[model_name])\n",
    "    metrics_by_modelname[model_name] = metrics\n",
    "    print(f\"Completed {model_name} metrics accumulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {k: {m: metrics_by_modelname[m][k] for m in model_run_names} for k in [\"avg_hellinger\", \"kld\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"avg_hellinger\"][\"Panda\"][\"prediction_horizon\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(metrics[\"avg_hellinger\"][\"Panda\"][\"prediction_horizon\"][3584].values())\n",
    "num_nones = sum(v is None for v in values)\n",
    "num_nans = sum(np.isnan(v) for v in values if v is not None)\n",
    "print(f\"Number of None values: {num_nones}\")\n",
    "print(f\"Number of NaN values: {num_nans}\")\n",
    "print(f\"Number of values: {len(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(metrics[\"kld\"][\"Panda\"][\"prediction_horizon\"][3584].values())\n",
    "num_nones = sum(v is None for v in values)\n",
    "num_nans = sum(np.isnan(v) for v in values if v is not None)\n",
    "print(f\"Number of None values: {num_nones}\")\n",
    "print(f\"Number of NaN values: {num_nans}\")\n",
    "print(f\"Number of values: {len(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_length = 3584\n",
    "horizon_name = \"prediction_horizon\"\n",
    "\n",
    "show_chronos_zs = False\n",
    "show_chronos_sft = True\n",
    "\n",
    "\n",
    "def filter_nans(values):\n",
    "    arr = [float(v) for v in values if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "    return np.array(arr, dtype=float)\n",
    "\n",
    "\n",
    "avg_hellinger = {}\n",
    "for model_key in model_run_names:\n",
    "    avg_hellinger[model_key] = filter_nans(metrics[\"avg_hellinger\"][model_key][horizon_name][pred_length].values())\n",
    "\n",
    "# colors = DEFAULT_COLORS\n",
    "colors = DEFAULT_COLORS[:4] + [\"#FFB5B8\"]\n",
    "print(colors)\n",
    "\n",
    "num_bins = 50\n",
    "plt.figure(figsize=(4, 4))\n",
    "all_hellinger = np.concatenate(list(avg_hellinger.values()))\n",
    "print(f\"min hellinger: {all_hellinger.min()}, max hellinger: {all_hellinger.max()}\")\n",
    "bins = np.histogram_bin_edges(all_hellinger, bins=num_bins)\n",
    "\n",
    "# Increase hatch linewidth so it appears in PDF\n",
    "plt.rcParams[\"hatch.linewidth\"] = 2.0\n",
    "\n",
    "alpha_val = 0.6\n",
    "\n",
    "for i, (label, vals) in enumerate(avg_hellinger.items()):\n",
    "    if not show_chronos_zs and label in [\"Chronos 200M\", \"Chronos 20M\"]:\n",
    "        continue\n",
    "    if not show_chronos_sft and label == \"Chronos 20M SFT\":\n",
    "        continue\n",
    "    if label == \"DynaMix\":\n",
    "        # For hatches to appear in PDF, we need to manually set the patches\n",
    "        n, bins_edges, patches = plt.hist(\n",
    "            vals,\n",
    "            bins=bins,\n",
    "            color=colors[i],\n",
    "            alpha=alpha_val,\n",
    "            label=label,\n",
    "            histtype=\"stepfilled\",\n",
    "            linewidth=2,\n",
    "            # zorder=9,\n",
    "            zorder=10 - i,\n",
    "            edgecolor=colors[i],\n",
    "        )\n",
    "        # Apply hatch to each patch with contrasting edge color\n",
    "        for patch in patches:\n",
    "            patch.set_hatch(\"////\")\n",
    "            patch.set_edgecolor(\"hotpink\")  # Use hotpink for hatch visibility\n",
    "    else:\n",
    "        plt.hist(\n",
    "            vals,\n",
    "            bins=bins,\n",
    "            color=colors[i],\n",
    "            edgecolor=colors[i],\n",
    "            alpha=alpha_val,\n",
    "            zorder=10 - i,\n",
    "            histtype=\"stepfilled\",\n",
    "            label=label,\n",
    "        )\n",
    "\n",
    "# plt.yscale(\"log\")\n",
    "plt.ylabel(\n",
    "    \"Count\",\n",
    "    fontweight=\"bold\",\n",
    "    # fontsize=14,\n",
    ")\n",
    "plt.legend(\n",
    "    loc=\"upper right\",\n",
    "    frameon=True,\n",
    "    fontsize=10,\n",
    "    # fontsize=12,\n",
    ")\n",
    "plt.title(\n",
    "    f\"Avg Hellinger ($L_{{\\\\mathrm{{pred}}}} = {pred_length}$) Last $2048$\",\n",
    "    fontweight=\"bold\",\n",
    "    # fontsize=15,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(fig_save_dir, f\"avg_hellinger_distribution_{horizon_name}_{pred_length}.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_length = 3584\n",
    "horizon_name = \"prediction_horizon\"\n",
    "\n",
    "show_chronos_zs = False\n",
    "\n",
    "\n",
    "# Extract and filter positive KL divergence values\n",
    "def pos_vals(vals):\n",
    "    return [x for x in vals if x is not None and x > 0]\n",
    "\n",
    "\n",
    "kld_dict = {}\n",
    "for model_key in model_run_names:\n",
    "    kld_dict[model_key] = pos_vals(metrics[\"kld\"][model_key][horizon_name][pred_length].values())\n",
    "\n",
    "all_kld_pos = np.concatenate(list(kld_dict.values()))\n",
    "num_bins = 50\n",
    "if len(all_kld_pos) > 0:\n",
    "    bins = np.linspace(all_kld_pos.min(), all_kld_pos.max(), num_bins)\n",
    "else:\n",
    "    bins = num_bins\n",
    "    print(\"No positive values found\")\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "# Increase hatch linewidth so it appears in PDF\n",
    "plt.rcParams[\"hatch.linewidth\"] = 2.0\n",
    "\n",
    "alpha_val = 0.6\n",
    "for i, (label, vals) in enumerate(kld_dict.items()):\n",
    "    # if not show_chronos_zs and label in [\"Chronos 200M\", \"Chronos 20M\"]:\n",
    "    if not show_chronos_zs and label in [\"Chronos 200M\", \"Chronos 20M\"]:\n",
    "        continue\n",
    "\n",
    "    # Also skip Chronos 200M to make the plot less crowded\n",
    "    if label == \"Chronos 200M\":\n",
    "        continue\n",
    "\n",
    "    if label == \"DynaMix\":\n",
    "        # For hatches to appear in PDF, we need to manually set the patches\n",
    "        n, bins_edges, patches = plt.hist(\n",
    "            vals,\n",
    "            bins=bins,\n",
    "            color=colors[i],\n",
    "            alpha=alpha_val,\n",
    "            label=label,\n",
    "            histtype=\"stepfilled\",\n",
    "            linewidth=2,\n",
    "            zorder=10 - i,\n",
    "            edgecolor=colors[i],\n",
    "        )\n",
    "        # Apply hatch to each patch with contrasting edge color\n",
    "        for patch in patches:\n",
    "            patch.set_hatch(\"////\")\n",
    "            patch.set_edgecolor(\"hotpink\")  # Use hotpink for hatch visibility\n",
    "    else:\n",
    "        plt.hist(\n",
    "            vals,\n",
    "            bins=bins,\n",
    "            color=colors[i],\n",
    "            edgecolor=colors[i],\n",
    "            alpha=alpha_val,\n",
    "            histtype=\"stepfilled\",\n",
    "            label=label,\n",
    "            zorder=10 - i,\n",
    "        )\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\n",
    "    \"Count\",\n",
    "    fontweight=\"bold\",\n",
    "    # fontsize=14,\n",
    ")\n",
    "plt.legend(\n",
    "    loc=\"upper right\",\n",
    "    frameon=True,\n",
    "    fontsize=10,\n",
    "    # fontsize=12,\n",
    ")\n",
    "plt.title(\n",
    "    f\"KL Divergence ($L_{{\\\\mathrm{{pred}}}} = {pred_length}$) Last $2048$\",\n",
    "    fontweight=\"bold\",\n",
    "    # fontsize=15,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(fig_save_dir, f\"kld_distribution_{horizon_name}_{pred_length}_log.pdf\"), bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_length = 3584\n",
    "horizon_name = \"prediction_horizon\"\n",
    "\n",
    "# Extract KL divergences for each model\n",
    "full_kld_dict = {\n",
    "    model_key: pos_vals(metrics[\"kld\"][model_key][horizon_name][pred_length].values()) for model_key in model_run_names\n",
    "}\n",
    "\n",
    "# Compute difference between Chronos SFT and Panda\n",
    "kld_diff = np.array(\n",
    "    [c - p for c, p in zip(full_kld_dict[\"Chronos 20M SFT\"], full_kld_dict[\"Panda\"]) if c is not None and p is not None]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.hist(kld_diff, bins=30, color=\"gray\", edgecolor=\"black\", alpha=0.7, histtype=\"stepfilled\")\n",
    "plt.axvline(0, color=\"k\", linestyle=\"dotted\", linewidth=1.5)\n",
    "plt.xlabel(\"$D_{{KL}}$ (Chronos SFT - Panda)\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Count\", fontweight=\"bold\")\n",
    "plt.title(f\"Difference in $D_{{KL}}$ ($L_{{\\\\mathrm{{pred}}}} = {pred_length}$)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_lengths = [128, 256, 512, 1024]\n",
    "pred_lengths = [512, 1024, 2048, 3072, 3584]\n",
    "horizon_name = \"prediction_horizon\"\n",
    "print(f\"horizon_name: {horizon_name}\")\n",
    "# Determine which Chronos base model is available\n",
    "\n",
    "pairs = [\n",
    "    (\"Chronos 20M SFT - Panda\", \"Chronos 20M SFT\", \"Panda\"),\n",
    "    (\"Chronos 20M - Chronos 20M SFT\", \"Chronos 20M\", \"Chronos 20M SFT\"),\n",
    "    (\"Chronos 200M - Chronos 20M SFT\", \"Chronos 200M\", \"Chronos 20M SFT\"),\n",
    "    (\"DynaMix - Panda\", \"DynaMix\", \"Panda\"),\n",
    "]\n",
    "for label, key1, key2 in pairs:\n",
    "    print(f\"{label} KLD Diff:\")\n",
    "    for pred_length in pred_lengths:\n",
    "        # Get all keys and ensure they exist\n",
    "        all_keys = set(model_run_names)\n",
    "        klds = {}\n",
    "        for key in all_keys:\n",
    "            klds[key] = np.array(list(metrics[\"kld\"][key][horizon_name][pred_length].values()))\n",
    "\n",
    "        # Filter out None values when computing difference\n",
    "        valid_pairs = [(v1, v2) for v1, v2 in zip(klds[key1], klds[key2]) if v1 is not None and v2 is not None]\n",
    "        if valid_pairs:\n",
    "            diff = np.array([v1 - v2 for v1, v2 in valid_pairs])\n",
    "            print(f\"  Prediction length {pred_length}: (mean ± std) = {diff.mean():.2f} ± {diff.std():.2f}\")\n",
    "        else:\n",
    "            print(f\"  Prediction length {pred_length}: No valid data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_lengths = [128, 256, 512, 1024]\n",
    "pred_lengths = [512, 1024, 2048, 3072, 3584]\n",
    "horizon_name = \"prediction_horizon\"\n",
    "print(f\"horizon_name: {horizon_name}\")\n",
    "# Determine which Chronos base model is available\n",
    "\n",
    "pairs = [\n",
    "    (\"Chronos 20M SFT - Panda\", \"Chronos 20M SFT\", \"Panda\"),\n",
    "    (\"Chronos 20M - Chronos 20M SFT\", \"Chronos 20M\", \"Chronos 20M SFT\"),\n",
    "    (\"Chronos 200M - Chronos 20M SFT\", \"Chronos 200M\", \"Chronos 20M SFT\"),\n",
    "    (\"DynaMix - Panda\", \"DynaMix\", \"Panda\"),\n",
    "]\n",
    "for label, key1, key2 in pairs:\n",
    "    print(f\"{label} Hellinger Diff:\")\n",
    "    for pred_length in pred_lengths:\n",
    "        # Get all keys and ensure they exist\n",
    "        all_keys = set(model_run_names)\n",
    "        hellingers = {}\n",
    "        for key in all_keys:\n",
    "            hellingers[key] = np.array(list(metrics[\"avg_hellinger\"][key][horizon_name][pred_length].values()))\n",
    "\n",
    "        # Filter out None values when computing difference\n",
    "        valid_pairs = [\n",
    "            (v1, v2) for v1, v2 in zip(hellingers[key1], hellingers[key2]) if v1 is not None and v2 is not None\n",
    "        ]\n",
    "        if valid_pairs:\n",
    "            diff = np.array([v1 - v2 for v1, v2 in valid_pairs])\n",
    "            # print(f\"Mean Hellinger diff ({label}): {diff.mean():.4f}, Std Hellinger diff: {diff.std():.4f}\")\n",
    "            print(f\"  Prediction length {pred_length}: (mean ± std) = {diff.mean():.2f} ± {diff.std():.2f}\")\n",
    "        else:\n",
    "            print(f\"No valid data for {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_lengths = [128, 256, 512, 1024]\n",
    "pred_lengths = [512, 1024, 2048, 3072, 3584]\n",
    "# horizon_names = [\"prediction_horizon\", \"full_trajectory\"]\n",
    "horizon_names = [\"prediction_horizon\"]\n",
    "for horizon_name in horizon_names:\n",
    "    print(\"-\" * 100)\n",
    "    print(\"KL Divergence\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"horizon_name: {horizon_name}\")\n",
    "    for model_key in model_run_names:\n",
    "        print(f\"Model: {model_key}\")\n",
    "        for pred_length in pred_lengths:\n",
    "            kld = list(metrics[\"kld\"][model_key][horizon_name][pred_length].values())\n",
    "            # Filter out None and NaN values\n",
    "            kld_values_filtered = [v for v in kld if v is not None and not np.isnan(v)]\n",
    "            if kld_values_filtered:\n",
    "                mean_kld = np.mean(kld_values_filtered)\n",
    "                std_kld = np.std(kld_values_filtered)\n",
    "                # print(f\"  Prediction length {pred_length}: mean kld = {mean_kld:.4f}, std kld = {std_kld:.4f}\")\n",
    "                print(f\"  Prediction length {pred_length}: (mean ± std) = {mean_kld:.2f} ± {std_kld:.2f}\")\n",
    "            else:\n",
    "                print(f\"  Prediction length {pred_length}: No valid data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_lengths = [128, 256, 512, 1024]\n",
    "pred_lengths = [512, 1024, 2048, 3072, 3584]\n",
    "# horizon_names = [\"prediction_horizon\", \"full_trajectory\"]\n",
    "horizon_names = [\"prediction_horizon\"]\n",
    "for horizon_name in horizon_names:\n",
    "    print(\"-\" * 100)\n",
    "    print(\"Avg Hellinger Distance\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"horizon_name: {horizon_name}\")\n",
    "    for model_key in model_run_names:\n",
    "        print(f\"Model: {model_key}\")\n",
    "        for pred_length in pred_lengths:\n",
    "            hell_values = list(metrics[\"avg_hellinger\"][model_key][horizon_name][pred_length].values())\n",
    "            # Filter out None and NaN values\n",
    "            hell_values_filtered = [v for v in hell_values if v is not None and not np.isnan(v)]\n",
    "            if hell_values_filtered:\n",
    "                mean_hell = np.mean(hell_values_filtered)\n",
    "                std_hell = np.std(hell_values_filtered)\n",
    "                # print(\n",
    "                #     f\"  Prediction length {pred_length}: mean avg_hellinger = {mean_hell:.4f}, std avg_hellinger = {std_hell:.4f}\"\n",
    "                # )\n",
    "                print(f\"  Prediction length {pred_length}: (mean ± std) = {mean_hell:.2f} ± {std_hell:.2f}\")\n",
    "            else:\n",
    "                print(f\"  Prediction length {pred_length}: No valid data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
