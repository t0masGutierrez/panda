{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from dysts.metrics import compute_metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "from panda.chronos.pipeline import ChronosPipeline\n",
    "from panda.patchtst.pipeline import PatchTSTPipeline\n",
    "from panda.utils.data_utils import safe_standardize\n",
    "from panda.utils.plot_utils import apply_custom_style\n",
    "\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_rank = 3\n",
    "device = torch.device(f\"cuda:{device_rank}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK = os.environ.get(\"WORK\", \"\")\n",
    "base_dir = f\"{WORK}/physics-datasets\"\n",
    "re = 450\n",
    "fpath = f\"{base_dir}/von_karman_street/vortex_street_velocities_Re_{re}_4800timepoints.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = \"../figures/vonkarman\"\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfield = np.load(fpath, allow_pickle=True)\n",
    "vx = vfield[..., 0]\n",
    "vy = vfield[..., 1]\n",
    "dvx = np.gradient(vx, 2 / vx.shape[1], axis=2)\n",
    "dvy = np.gradient(vy, 1 / vy.shape[2], axis=1)\n",
    "vort_field = dvy + dvx\n",
    "vort_field_flattened = vort_field.reshape(vort_field.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 512\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(vort_field_flattened)\n",
    "X_ts = pca.transform(vort_field_flattened)  # (T, D)\n",
    "eigenvectors = pca.components_  # (D, H*W)\n",
    "\n",
    "## Show low-rank structure\n",
    "plt.figure()\n",
    "plt.plot(np.arange(n_components), pca.explained_variance_ratio_)\n",
    "plt.semilogy()\n",
    "\n",
    "## Plot trajectory\n",
    "plt.figure()\n",
    "plt.plot(X_ts[:, 0], X_ts[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(pca_coeffs: np.ndarray, eigenvectors: np.ndarray, modes: int = -1) -> np.ndarray:\n",
    "    if modes == -1:\n",
    "        modes = pca_coeffs.shape[1]\n",
    "    return pca_coeffs[:, :modes] @ eigenvectors[:modes, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vort_recon = reconstruct(X_ts, eigenvectors)\n",
    "vort_recon = vort_recon.reshape(vort_field.shape[0], vort_field.shape[1], vort_field.shape[2])\n",
    "plt.figure()\n",
    "plt.imshow(vort_recon[100 + 512, :, :].T, cmap=\"seismic\")\n",
    "plt.colorbar(shrink=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"pft_chattn_emb_w_poly-0\"\n",
    "pft_model = PatchTSTPipeline.from_pretrained(\n",
    "    mode=\"predict\", pretrain_path=f\"/stor/work/Gilpin/checkpoints/{run_name}/checkpoint-final\", device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_ft = ChronosPipeline.from_pretrained(\n",
    "    \"/stor/work/Gilpin/checkpoints/chronos_t5_mini_ft-0/checkpoint-final\",\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_zs = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-mini\",\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralop.models import FNO\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def fno_rollout(\n",
    "    context: np.ndarray,\n",
    "    num_steps: int,\n",
    "    model: torch.nn.Module,\n",
    "    lookback: int,\n",
    "    eigenvectors: np.ndarray,\n",
    "    shape: tuple[int, int] = (vort_field.shape[1], vort_field.shape[2]),\n",
    "    standardize: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    if standardize:\n",
    "        ic = safe_standardize(context, axis=0)[-lookback:]\n",
    "    else:\n",
    "        ic = context[-lookback:]\n",
    "    ic = torch.tensor(ic).float().to(device).unsqueeze(0)\n",
    "    traj = [ic]\n",
    "    state = ic.clone()\n",
    "    for _ in range(num_steps):\n",
    "        pred = model(state)\n",
    "        traj.append(pred)\n",
    "        state = torch.cat([state[:, :-1, :], pred], dim=1)\n",
    "    pred = torch.cat(traj, dim=1)[:, lookback:, :].cpu().numpy().squeeze()\n",
    "    if standardize:\n",
    "        pred = safe_standardize(pred, axis=0, context=context, denormalize=True)\n",
    "    return reconstruct(pred, eigenvectors).reshape(-1, *shape)\n",
    "\n",
    "\n",
    "lookback = 8\n",
    "operator = FNO(n_modes=(512,), hidden_channels=256, n_layers=5, in_channels=lookback, out_channels=1).to(device)\n",
    "operator.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"{WORK}/checkpoints/VKVS-baselines-full/best_model_state_dict.pt\", map_location=device, weights_only=False\n",
    "    )\n",
    ")\n",
    "operator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepxde as dde\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def deeponet_rollout(\n",
    "    context: np.ndarray,\n",
    "    num_steps: int,\n",
    "    model: torch.nn.Module,\n",
    "    xs: np.ndarray,\n",
    "    eigenvectors: np.ndarray,\n",
    "    shape: tuple[int, int] = (vort_field.shape[1], vort_field.shape[2]),\n",
    "    standardize: bool = True,\n",
    "):\n",
    "    if standardize:\n",
    "        ic = safe_standardize(context, axis=0)[-1]\n",
    "    else:\n",
    "        ic = context[-1]\n",
    "    ic = torch.tensor(ic).float().to(device).unsqueeze(0)\n",
    "    traj = [ic]\n",
    "    state = ic.clone()\n",
    "    for _ in range(num_steps):\n",
    "        state = model((state, xs))\n",
    "        traj.append(state)\n",
    "    pred = torch.cat(traj, dim=0)[1:].cpu().numpy().squeeze()\n",
    "    if standardize:\n",
    "        pred = safe_standardize(pred, axis=0, context=context, denormalize=True)\n",
    "    return reconstruct(pred, eigenvectors).reshape(-1, *shape)\n",
    "\n",
    "\n",
    "xs = np.linspace(0, 1, n_components, endpoint=False).astype(np.float32).reshape(-1, 1)  # (N,1)\n",
    "xs = torch.from_numpy(xs).float().to(device)\n",
    "\n",
    "hidden_dim = 512\n",
    "n_layers = 5\n",
    "net = dde.nn.DeepONetCartesianProd(\n",
    "    [n_components] + n_layers * [hidden_dim],\n",
    "    [1] + n_layers * [hidden_dim],\n",
    "    \"tanh\",\n",
    "    \"He normal\",\n",
    ").to(device)\n",
    "state_dict = torch.load(f\"{WORK}/checkpoints/VKVS-baselines-full/deeponet.pt\", map_location=device, weights_only=False)[\n",
    "    \"model_state_dict\"\n",
    "]\n",
    "net.load_state_dict(state_dict)\n",
    "net.eval()\n",
    "torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forecast(\n",
    "    model: torch.nn.Module,\n",
    "    context: np.ndarray,\n",
    "    prediction_length: int,\n",
    "    eigenvectors: np.ndarray,\n",
    "    batch_size: int | None = None,\n",
    "    shape: tuple[int, int] = vort_field.shape[1:3],\n",
    "    num_modes: int | None = None,\n",
    "    transpose: bool = False,\n",
    "    standardize: bool = True,\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if standardize:\n",
    "        context_tensor = torch.from_numpy(safe_standardize(context, axis=0)).float()\n",
    "    else:\n",
    "        context_tensor = torch.from_numpy(context).float()\n",
    "\n",
    "    if transpose:\n",
    "        context_tensor = context_tensor.T\n",
    "\n",
    "    if batch_size is None:\n",
    "        pred = model.predict(context_tensor, prediction_length, **kwargs)\n",
    "    else:\n",
    "        pred = []\n",
    "        for i in range(0, context_tensor.shape[0], batch_size):\n",
    "            pred.append(model.predict(context_tensor[i : i + batch_size], prediction_length, **kwargs))\n",
    "        pred = torch.cat(pred, dim=0)\n",
    "\n",
    "    pred = pred.squeeze().detach().cpu().numpy()\n",
    "    if transpose:\n",
    "        pred = pred.T\n",
    "\n",
    "    if standardize:\n",
    "        pred = safe_standardize(\n",
    "            pred,\n",
    "            axis=0,\n",
    "            context=context,\n",
    "            denormalize=True,\n",
    "        )\n",
    "\n",
    "    recon = reconstruct(pred, eigenvectors, modes=num_modes)\n",
    "    recon = recon.reshape(prediction_length, shape[0], shape[1])\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_flow(\n",
    "    prediction: np.ndarray,\n",
    "    groundtruth: np.ndarray,\n",
    "    time_indices: list[int],\n",
    "    shape: tuple[int, int] = (vort_field.shape[1], vort_field.shape[2]),\n",
    "    save_path: str | None = None,\n",
    "    cmap_name=\"seismic\",\n",
    "    base_figsize=(5, 5),\n",
    "    suptitle: str | None = None,\n",
    "    suptitle_y: float = 1.0,\n",
    "):\n",
    "    vabs = max(np.abs(groundtruth.min()), np.abs(groundtruth.max()))\n",
    "\n",
    "    aspect_ratio = shape[0] / shape[1]\n",
    "    fig = plt.figure(\n",
    "        figsize=(\n",
    "            base_figsize[0] * (len(time_indices)) / aspect_ratio,\n",
    "            base_figsize[1] * aspect_ratio,\n",
    "        )\n",
    "    )\n",
    "    gs = fig.add_gridspec(\n",
    "        2,\n",
    "        len(time_indices),\n",
    "        width_ratios=[1] * (len(time_indices)),\n",
    "        height_ratios=[1, 1],\n",
    "        wspace=0,\n",
    "        hspace=0,\n",
    "    )\n",
    "    axes = np.array([[fig.add_subplot(gs[i, j]) for j in range(len(time_indices))] for i in range(2)])\n",
    "    for i, index in enumerate(time_indices):\n",
    "        groundtruth_slice = groundtruth[index, :, :]\n",
    "        recon_slice = prediction[index, :, :]\n",
    "\n",
    "        gax = axes[0, i].imshow(\n",
    "            groundtruth_slice,\n",
    "            vmin=-vabs,\n",
    "            vmax=vabs,\n",
    "            cmap=cmap_name,\n",
    "        )\n",
    "\n",
    "        axes[0, i].spines[\"top\"].set_visible(True)\n",
    "        axes[0, i].spines[\"right\"].set_visible(True)\n",
    "        axes[0, i].spines[\"bottom\"].set_visible(True)\n",
    "        axes[0, i].spines[\"left\"].set_visible(True)\n",
    "        axes[0, i].spines[\"top\"].set_color(\"black\")\n",
    "        axes[0, i].spines[\"right\"].set_color(\"black\")\n",
    "        axes[0, i].spines[\"bottom\"].set_color(\"black\")\n",
    "        axes[0, i].spines[\"left\"].set_color(\"black\")\n",
    "\n",
    "        axes[0, i].set_title(\n",
    "            # f\"t={context_length}\" + (f\" + {index}\" if index > 0 else \"\"),\n",
    "            \"t = context\" + (f\" + {index + 1}\" if index > 0 else \"\"),\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "\n",
    "        rax = axes[1, i].imshow(\n",
    "            recon_slice,\n",
    "            vmin=-vabs,\n",
    "            vmax=vabs,\n",
    "            cmap=cmap_name,\n",
    "        )\n",
    "        axes[1, i].spines[\"top\"].set_visible(True)\n",
    "        axes[1, i].spines[\"right\"].set_visible(True)\n",
    "        axes[1, i].spines[\"bottom\"].set_visible(True)\n",
    "        axes[1, i].spines[\"left\"].set_visible(True)\n",
    "        axes[1, i].spines[\"top\"].set_color(\"black\")\n",
    "        axes[1, i].spines[\"right\"].set_color(\"black\")\n",
    "        axes[1, i].spines[\"bottom\"].set_color(\"black\")\n",
    "        axes[1, i].spines[\"left\"].set_color(\"black\")\n",
    "\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "\n",
    "        circle = plt.Circle((0.5 * shape[1] + 1, 0.145 * shape[0]), 5, fill=True, color=\"black\")\n",
    "        axes[0, i].add_patch(circle)\n",
    "        circle = plt.Circle((0.5 * shape[1] + 1, 0.145 * shape[0]), 5, fill=True, color=\"black\")\n",
    "        axes[1, i].add_patch(circle)\n",
    "\n",
    "    axes[0, 0].set_ylabel(\"Ground Truth\", fontweight=\"bold\", fontsize=16)\n",
    "    axes[1, 0].set_ylabel(\"Predictions\", fontweight=\"bold\", fontsize=16)\n",
    "\n",
    "    if suptitle is not None:\n",
    "        plt.suptitle(suptitle, fontweight=\"bold\", fontsize=18, ha=\"center\", y=suptitle_y)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 1\n",
    "start = 2048  # ignore transient\n",
    "context_length = 512\n",
    "prediction_length = 128\n",
    "time_indices = [15, 31, 63, 127]\n",
    "\n",
    "strided_X = X_ts[start::stride]\n",
    "groundtruth = reconstruct(strided_X[context_length : context_length + prediction_length], eigenvectors).reshape(\n",
    "    -1, *vort_field.shape[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_prediction = forecast(\n",
    "    pft_model,\n",
    "    strided_X[:context_length],\n",
    "    prediction_length,\n",
    "    eigenvectors,\n",
    "    sliding_context=True,\n",
    "    limit_prediction_length=False,\n",
    ")\n",
    "plot_predicted_flow(\n",
    "    panda_prediction,\n",
    "    groundtruth,\n",
    "    time_indices=time_indices,\n",
    "    cmap_name=\"RdBu\",\n",
    "    base_figsize=(4, 4),\n",
    "    save_path=os.path.join(figs_save_dir, \"von_karman_our_model.pdf\"),\n",
    "    suptitle=\"von K치rm치n Vortex Street\",\n",
    "    suptitle_y=0.98,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_ft_prediction = forecast(\n",
    "    chronos_ft,\n",
    "    strided_X[:context_length],\n",
    "    prediction_length,\n",
    "    eigenvectors,\n",
    "    batch_size=256,\n",
    "    transpose=True,\n",
    "    limit_prediction_length=False,\n",
    "    num_samples=1,\n",
    "    deterministic=True,\n",
    ")\n",
    "plot_predicted_flow(\n",
    "    chronos_ft_prediction,\n",
    "    groundtruth,\n",
    "    time_indices=time_indices,\n",
    "    cmap_name=\"RdBu\",\n",
    "    save_path=os.path.join(figs_save_dir, \"von_karman_chronos_ft.pdf\"),\n",
    "    suptitle=\"Chronos Finetune\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_zs_prediction = forecast(\n",
    "    chronos_zs,\n",
    "    strided_X[:context_length],\n",
    "    prediction_length,\n",
    "    eigenvectors,\n",
    "    batch_size=256,\n",
    "    transpose=True,\n",
    "    limit_prediction_length=False,\n",
    "    num_samples=1,\n",
    "    deterministic=True,\n",
    ")\n",
    "plot_predicted_flow(\n",
    "    chronos_zs_prediction,\n",
    "    groundtruth,\n",
    "    time_indices=time_indices,\n",
    "    cmap_name=\"RdBu\",\n",
    "    save_path=os.path.join(figs_save_dir, \"von_karman_chronos_zs.pdf\"),\n",
    "    suptitle=\"Chronos ZS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fno_prediction = fno_rollout(strided_X[:context_length], prediction_length, operator, lookback, eigenvectors)\n",
    "plot_predicted_flow(\n",
    "    fno_prediction,\n",
    "    groundtruth,\n",
    "    time_indices=time_indices,\n",
    "    cmap_name=\"RdBu\",\n",
    "    save_path=os.path.join(figs_save_dir, \"von_karman_fno.pdf\"),\n",
    "    suptitle=\"FNO\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_prediction = deeponet_rollout(strided_X[:context_length], prediction_length, net, xs, eigenvectors)\n",
    "plot_predicted_flow(\n",
    "    don_prediction,\n",
    "    groundtruth,\n",
    "    time_indices=time_indices,\n",
    "    cmap_name=\"RdBu\",\n",
    "    save_path=os.path.join(figs_save_dir, \"von_karman_don.pdf\"),\n",
    "    suptitle=\"DeepONet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Rollout Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rollout_metrics(\n",
    "    prediction: np.ndarray,\n",
    "    groundtruth: np.ndarray,\n",
    "    time_interval: int = 64,\n",
    "):\n",
    "    prediction_length = prediction.shape[0]\n",
    "    metrics = {}\n",
    "    rollout_time_intervals = np.arange(time_interval, prediction_length + time_interval, time_interval)\n",
    "    for t in rollout_time_intervals:\n",
    "        metrics[t] = compute_metrics(prediction[0:t], groundtruth[0:t], include=[\"mae\", \"mse\", \"smape\"])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = np.arange(0, 4096, 1024)\n",
    "n_runs = len(start_times)\n",
    "start_times = start_times.astype(int)\n",
    "print(f\"{len(start_times)} start_times: {start_times}\")\n",
    "\n",
    "prediction_length = 512\n",
    "use_chronos_deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"../outputs/vonkarman/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_metrics_path = os.path.join(save_dir, f\"{run_name}_preds_{n_runs}.npy\")\n",
    "if os.path.exists(our_metrics_path):\n",
    "    our_model_rollout_metrics = np.load(our_metrics_path, allow_pickle=True).item()\n",
    "else:\n",
    "    our_model_rollout_metrics = defaultdict(dict)\n",
    "    for start_time in tqdm(start_times, desc=\"Running rollouts\"):\n",
    "        context_start = start_time + context_length\n",
    "        end = context_start + prediction_length\n",
    "        groundtruth = reconstruct(X_ts[context_start:end:stride], eigenvectors).reshape(-1, *vort_field.shape[1:])\n",
    "        panda_prediction = forecast(\n",
    "            pft_model,\n",
    "            X_ts[start_time:context_start:stride],\n",
    "            prediction_length,\n",
    "            eigenvectors,\n",
    "            sliding_context=True,\n",
    "            limit_prediction_length=False,\n",
    "        )\n",
    "        metrics = compute_rollout_metrics(panda_prediction, groundtruth, time_interval=64)\n",
    "        for t, metric in metrics.items():\n",
    "            for metric_name, metric_val in metric.items():\n",
    "                if t not in our_model_rollout_metrics[metric_name]:\n",
    "                    our_model_rollout_metrics[metric_name][t] = []\n",
    "                our_model_rollout_metrics[metric_name][t].append(metric_val)\n",
    "\n",
    "    np.save(our_metrics_path, our_model_rollout_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_ft_metrics_path = os.path.join(\n",
    "    save_dir,\n",
    "    f\"chronos_ft_preds_{n_runs}{'' if use_chronos_deterministic else '_nondeterministic'}.npy\",\n",
    ")\n",
    "if os.path.exists(chronos_ft_metrics_path):\n",
    "    chronos_ft_rollout_metrics = np.load(chronos_ft_metrics_path, allow_pickle=True).item()\n",
    "else:\n",
    "    chronos_ft_rollout_metrics = defaultdict(dict)\n",
    "    for start_time in tqdm(start_times, desc=\"Running rollouts\"):\n",
    "        context_start = start_time + context_length\n",
    "        end = context_start + prediction_length\n",
    "        groundtruth = reconstruct(X_ts[context_start:end:stride], eigenvectors).reshape(-1, *vort_field.shape[1:])\n",
    "        chronos_ft_prediction = forecast(\n",
    "            chronos_ft,\n",
    "            X_ts[start_time:context_start:stride],\n",
    "            prediction_length,\n",
    "            eigenvectors,\n",
    "            batch_size=256,\n",
    "            transpose=True,\n",
    "            limit_prediction_length=False,\n",
    "            num_samples=1,\n",
    "            deterministic=use_chronos_deterministic,\n",
    "        )\n",
    "        metrics_chronos_ft = compute_rollout_metrics(chronos_ft_prediction, groundtruth, time_interval=64)\n",
    "        for t, metric in metrics_chronos_ft.items():\n",
    "            for metric_name, metric_val in metric.items():\n",
    "                if t not in chronos_ft_rollout_metrics[metric_name]:\n",
    "                    chronos_ft_rollout_metrics[metric_name][t] = []\n",
    "                chronos_ft_rollout_metrics[metric_name][t].append(metric_val)\n",
    "\n",
    "    np.save(chronos_ft_metrics_path, chronos_ft_rollout_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_zs_metrics_path = os.path.join(\n",
    "    save_dir,\n",
    "    f\"chronos_zs_preds_{n_runs}{'' if use_chronos_deterministic else '_nondeterministic'}.npy\",\n",
    ")\n",
    "if os.path.exists(chronos_zs_metrics_path):\n",
    "    chronos_zs_rollout_metrics = np.load(chronos_zs_metrics_path, allow_pickle=True).item()\n",
    "else:\n",
    "    chronos_zs_rollout_metrics = defaultdict(dict)\n",
    "    for start_time in tqdm(start_times, desc=\"Running rollouts\"):\n",
    "        context_start = start_time + context_length\n",
    "        end = context_start + prediction_length\n",
    "        groundtruth = reconstruct(X_ts[context_start:end:stride], eigenvectors).reshape(-1, *vort_field.shape[1:])\n",
    "        chronos_zs_prediction = forecast(\n",
    "            chronos_zs,\n",
    "            X_ts[start_time:context_start:stride],\n",
    "            prediction_length,\n",
    "            eigenvectors,\n",
    "            batch_size=256,\n",
    "            transpose=True,\n",
    "            limit_prediction_length=False,\n",
    "            num_samples=1,\n",
    "            deterministic=use_chronos_deterministic,\n",
    "        )\n",
    "        metrics_chronos_zs = compute_rollout_metrics(chronos_zs_prediction, groundtruth, time_interval=64)\n",
    "        for t, metric_dict in metrics_chronos_zs.items():\n",
    "            for metric_name, metric_val in metric_dict.items():\n",
    "                if t not in chronos_zs_rollout_metrics[metric_name]:\n",
    "                    chronos_zs_rollout_metrics[metric_name][t] = []\n",
    "                chronos_zs_rollout_metrics[metric_name][t].append(metric_val)\n",
    "\n",
    "    np.save(chronos_zs_metrics_path, chronos_zs_rollout_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fno_metrics_path = os.path.join(save_dir, f\"fno_preds_{n_runs}.npy\")\n",
    "if os.path.exists(fno_metrics_path):\n",
    "    fno_rollout_metrics = np.load(fno_metrics_path, allow_pickle=True).item()\n",
    "else:\n",
    "    fno_rollout_metrics = defaultdict(dict)\n",
    "    for start_time in tqdm(start_times, desc=\"Running rollouts\"):\n",
    "        context_start = start_time + context_length\n",
    "        end = context_start + prediction_length\n",
    "        groundtruth = reconstruct(X_ts[context_start:end:stride], eigenvectors).reshape(-1, *vort_field.shape[1:])\n",
    "        fno_prediction = fno_rollout(\n",
    "            X_ts[start_time:context_start:stride], prediction_length, operator, lookback, eigenvectors\n",
    "        )\n",
    "        metrics_fno = compute_rollout_metrics(fno_prediction, groundtruth, time_interval=64)\n",
    "        for t, metric_dict in metrics_fno.items():\n",
    "            for metric_name, metric_val in metric_dict.items():\n",
    "                if t not in fno_rollout_metrics[metric_name]:\n",
    "                    fno_rollout_metrics[metric_name][t] = []\n",
    "                fno_rollout_metrics[metric_name][t].append(metric_val)\n",
    "\n",
    "    np.save(fno_metrics_path, fno_rollout_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_metrics_path = os.path.join(save_dir, f\"don_preds_{n_runs}.npy\")\n",
    "if os.path.exists(don_metrics_path):\n",
    "    don_rollout_metrics = np.load(don_metrics_path, allow_pickle=True).item()\n",
    "else:\n",
    "    don_rollout_metrics = defaultdict(dict)\n",
    "    for start_time in tqdm(start_times, desc=\"Running rollouts\"):\n",
    "        context_start = start_time + context_length\n",
    "        end = context_start + prediction_length\n",
    "        groundtruth = reconstruct(X_ts[context_start:end:stride], eigenvectors).reshape(-1, *vort_field.shape[1:])\n",
    "        don_prediction = deeponet_rollout(\n",
    "            X_ts[start_time:context_start:stride], prediction_length, net, xs, eigenvectors\n",
    "        )\n",
    "        metrics_don = compute_rollout_metrics(don_prediction, groundtruth, time_interval=64)\n",
    "        for t, metric_dict in metrics_don.items():\n",
    "            for metric_name, metric_val in metric_dict.items():\n",
    "                if t not in don_rollout_metrics[metric_name]:\n",
    "                    don_rollout_metrics[metric_name][t] = []\n",
    "                don_rollout_metrics[metric_name][t].append(metric_val)\n",
    "\n",
    "    np.save(don_metrics_path, don_rollout_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(data_dict):\n",
    "    mean_vals = {t: np.mean(v) for t, v in data_dict.items()}\n",
    "    median_vals = {t: np.median(v) for t, v in data_dict.items()}\n",
    "    std_vals = {t: np.std(v) for t, v in data_dict.items()}\n",
    "    ste_vals = {t: std_vals[t] / np.sqrt(len(data_dict[t])) for t in data_dict.keys()}\n",
    "    return mean_vals, median_vals, std_vals, ste_vals\n",
    "\n",
    "\n",
    "def plot_model_results(\n",
    "    mean_dict: dict[str, list[float]], ste_dict: dict[str, list[float]], marker: str, label: str, color: str, **kwargs\n",
    "):\n",
    "    x_values = list(mean_dict.keys())\n",
    "    y_values = list(mean_dict.values())\n",
    "    y_errors = list(ste_dict.values())\n",
    "\n",
    "    plt.plot(x_values, y_values, marker=marker, label=label, color=color, **kwargs)\n",
    "    plt.fill_between(\n",
    "        x_values,\n",
    "        np.array(y_values) - np.array(y_errors),\n",
    "        np.array(y_values) + np.array(y_errors),\n",
    "        alpha=0.1,\n",
    "        color=color,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.5, 3.5))\n",
    "\n",
    "markers = [\"o\", \"s\", \"v\", \"^\", \"x\"]\n",
    "model_names = [\"Panda\", \"Chronos 20M SFT\", \"Chronos 20M\", \"FNO\", \"DeepONet\"]\n",
    "rollout_metrics = [\n",
    "    our_model_rollout_metrics,\n",
    "    chronos_ft_rollout_metrics,\n",
    "    chronos_zs_rollout_metrics,\n",
    "    fno_rollout_metrics,\n",
    "    don_rollout_metrics,\n",
    "]\n",
    "for model, metrics, marker in zip(model_names, rollout_metrics, markers):\n",
    "    means, medians, stds, stes = calculate_stats(metrics[\"mae\"])\n",
    "    linestyle = \"-.\" if model in (\"FNO\", \"DeepONet\") else \"-\"\n",
    "    color = \"k\" if model in (\"FNO\", \"DeepONet\") else plt.gca()._get_lines.get_next_color()\n",
    "    plot_model_results(means, stes, marker, label=model, linestyle=linestyle, color=color)\n",
    "    plt.xlabel(\"Prediction Length\", fontweight=\"bold\")\n",
    "    plt.xticks(list(means.keys()))\n",
    "    plt.title(\"von K치rm치n Vortex Street\", fontweight=\"bold\")\n",
    "    plt.ylabel(\"MAE\", fontweight=\"bold\")\n",
    "    plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "\n",
    "plt.legend(loc=\"center left\", frameon=True, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(figs_save_dir, \"vonkarman_all_models_mae.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_combined = {\n",
    "    \"Panda\": our_model_rollout_metrics,\n",
    "    \"Chronos 20M SFT\": chronos_ft_rollout_metrics,\n",
    "    \"Chronos 20M\": chronos_zs_rollout_metrics,\n",
    "    \"FNO\": fno_rollout_metrics,\n",
    "    \"DeepONet\": don_rollout_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = \"../outputs/vonkarman\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, \"vonkarman_metrics_combined.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metrics_combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
