eval:
  # for evaluating saved checkpoint
  mode: predict
  data_paths_lst: null

  checkpoint_path: null
  device: cuda:7
  torch_dtype: float32
  batch_size: 32
  dataloader_num_workers: 1
  num_subdirs: null
  num_samples_per_subdir: null

  # inference mode for patchtst
  sliding_context: true

  save_contexts: false
  save_labels: false
  save_forecasts: false  # for forecasting model eval
  save_completions: false  # for mlm model eval
  save_masks: false  # for mlm model eval

  num_processes: 10 # for multiprocessing
  use_multiprocessing: true

  metric_names:
    - mse
    - mae
    - smape
    - spearman

  forecast_save_dir: null
  labels_save_dir: null
  completions_save_dir: null
  patch_input_save_dir: null
  timestep_masks_save_dir: null

  metrics_save_dir: null
  metrics_fname: metrics
  metrics_fname_suffix: null
  distributional_metrics_group: fdiv
  distributional_metrics_predlengths: [1024]
  horizons_lst: ["prediction_horizon"]
  use_kld_gmm: false
  fdiv_metrics_to_compute: ["avg_hellinger_distance", "kl_divergence"]

  overwrite: false
  seed: 1

  # for generating forecasts and getting test split
  num_samples: 1 # NOTE: this is only used in chronos/evaluate.py and scripts/compute_distributional_metrics.py
  parallel_sample_reduction: median
  limit_prediction_length: true
  context_length: 512
  prediction_length: 64

  # window style can be either sampled or rolling
  # sampled: randomly samples windows from each timeseries (of length context_length) 
  # rolling: takes consecutive windows of context_length with a stride of
  #   window_stride from each timeseries
  # (in this case, num_test_instances need not be specified - always number of sliding windows
  #  conditioned on window stride, namely (T - context_length - prediction_length) // window_stride + 1)
  # single: takes single window of length context_length starting from `window_start` or 0 if not specified
  # (in this case, num_test_instances need not be specified - always 1)
  num_test_instances: 1 # number of context windows to use for evaluation
  window_style: sampled
  window_stride: 1
  window_start: null
  split_coords: false
  verbose: false

  compute_distributional_metrics: true
  compute_gp_dims: true
  compute_naive_interpolations: false
  naive_interpolation_method: polynomial
  naive_interpolation_polynomial_degree: 3
  naive_interpolation_piecewise_spline_degree: 3
  debug_mode: false

  baselines:
    baseline_model: fourier_arima
    order: [4, 1, 4]  # (p, d, q) for ARIMA
    num_fourier_terms: 5

  chronos:
    zero_shot: false
    deterministic: true

  completions:
    start_time: 0
    end_time: null

  model_type: panda
  save_full_trajectory: false
  
  recompute_forecasts: false
  reload_saved_completions: false

# wandb run metrics, not to be confused with dysts eval metrics
run_metrics:
  wandb_run_id: null
  plot_dir: figures
  save_dir: null
  save_fname: metrics.json
